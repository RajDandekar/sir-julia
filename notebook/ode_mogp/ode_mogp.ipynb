{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian process surrogate model of an ordinary differential equation model\nSimon Frost (@sdwfrost), 2022-03-17\n\n## Introduction\n\nThis tutorial uses the Python package [mogp-emulator](https://github.com/alan-turing-institute/mogp-emulator) to train a Gaussian process emulator for the final size of an epidemic, with both the infectivity parameter, β, and the per-capita recovery rate, γ, allowed to vary.\n\n## Libraries"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using OrdinaryDiffEq\nusing DiffEqCallbacks\nusing Surrogates\nusing Conda\nusing PyCall\nusing Random\nusing Plots\nusing BenchmarkTools;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code (which only needs to be run once) installs the `mogp-emulator` package into a local Conda environment."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "env = Conda.ROOTENV\nConda.pip_interop(true, env)\nConda.pip(\"install\", \"mogp-emulator\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now import the Python packages."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "random = pyimport(\"random\")\nnp = pyimport(\"numpy\")\nmogp = pyimport(\"mogp_emulator\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For reproducibility, we set the Julia random seed, the Python seed and the numpy random seed."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(123)\nrandom.seed(123)\nnp.random.seed(123);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transitions\n\nThis is the standard ODE model widely used in this repository, with the exception that we collapse infectivity, the (constant) population size, N, and the contact rate into a single parameter, β."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function sir_ode!(du,u,p,t)\n    (S,I,R) = u\n    (β,γ) = p\n    @inbounds begin\n        du[1] = -β*S*I\n        du[2] = β*S*I - γ*I\n        du[3] = γ*I\n    end\n    nothing\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time domain\n\nWe set the maximum time to be high as we will stop the simulation via a callback."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "tmax = 10000.0\ntspan = (0.0,tmax)\nδt = 1.0;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial conditions\n\nWe need to run the model for lots of initial conditions and parameter values."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "n_train = 50 # Number of training samples\nn_test = 1000; # Number of test samples"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We specify lower (`lb`) and upper (`ub`) bounds for each parameter."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Parameters are β, γ\nlb = [0.00005, 0.1]\nub = [0.001, 1.0];"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the model\n\nOur simulation function will make use of a pre-defined `ODEProblem`, which we define here along with default parameter values."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "N = 1000.0\nu0 = [990.0,10.0,0.0]\np = [0.0005,0.25]\nprob_ode = ODEProblem(sir_ode!,u0,tspan,p);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a surrogate model\n\nWe start by sampling values of β between the lower and upper bounds using Latin hypercube sampling (via Surrogates.jl), which will give more uniform coverage than a uniform sample given the low number of initial points."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sampler = LatinHypercubeSample();"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "θ = Surrogates.sample(n_train,lb,ub,sampler);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian processes do not restrict values to be positive; however, final size is bounded by 0 and 1. Hence, we consider a logit-transformed final size obtained by running the model until it reaches steady state."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "logit = (x) -> log(x/(1-x))\ninvlogit = (x) -> exp(x)/(exp(x)+1.0)\ncb_ss = TerminateSteadyState()\nlogit_final_size = function(z)\n  prob = remake(prob_ode;p=z)\n  sol = solve(prob, ROS34PW3(),callback=cb_ss)\n  fsp = sol[end][3]/N\n  logit(fsp)\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now calculate the logit final size as follows."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lfs = logit_final_size.(θ);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function call passes the array of input parameters, θ, and the array of logit-transformed final sizes, `lfs` to the `GaussianProcess` class in the Python `mogp-emulator` package, which assumes a single target variable."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gp = mogp.GaussianProcess(θ, lfs, nugget=\"fit\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have instantiated the Gaussian process, we can fit using maximum a posteriori (MAP) optimization. We will use multiple tries in order to get a good-fitting model. Many tries will generate errors."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gp = mogp.fit_GP_MAP(gp, n_tries=100);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following automatically converts the output of the Python predict function to Julia."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "lfs_train_pred = gp.predict(θ);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter(invlogit.(lfs),\n        invlogit.(lfs_train_pred[\"mean\"]),\n        xlabel = \"Model final size\",\n        ylabel = \"Surrogate final size\",\n        legend = false,\n        title = \"Training set\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have fitted the Gaussian process, we can evaluate on a larger set of test parameters."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "θ_test = sample(n_test,lb,ub,sampler)\nlfs_test = logit_final_size.(θ_test)\nlfs_test_pred = gp.predict(θ_test);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output gives a reasonable approximation of the model output."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter(invlogit.(lfs_test),\n        invlogit.(lfs_test_pred[\"mean\"]),\n        xlabel = \"Model final size\",\n        ylabel = \"Surrogate final size\",\n        legend = false,\n        title = \"Test set\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain further insights, we can fix one of the parameters while sweeping over a fine grid of the other. Firstly, we fix the recovery rate γ and vary β."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "β_grid = collect(lb[1]:0.00001:ub[1])\nθ_eval = [[βᵢ,0.25] for βᵢ in β_grid]\nlfs_eval = gp.predict(θ_eval)\nfs_eval = invlogit.(lfs_eval[\"mean\"])\nfs_eval_uc = invlogit.(lfs_eval[\"mean\"] .+ 1.96 .* sqrt.(lfs_eval[\"unc\"]))\nfs_eval_lc = invlogit.(lfs_eval[\"mean\"] .- 1.96 .* sqrt.(lfs_eval[\"unc\"]))\nplot(β_grid,\n     fs_eval,\n     xlabel = \"Infectivity parameter, β\",\n     ylabel = \"Final size\",\n     label = \"Model\")\nplot!(β_grid,\n      invlogit.(logit_final_size.(θ_eval)),\n      ribbon = (fs_eval .- fs_eval_lc, fs_eval_uc - fs_eval),\n      label = \"Surrogate\",\n      legend = :right)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the above, for a range of values of β, the true value of the model lies outside of the uncertainty range of the emulator.\n\nNow, we fix β and vary the recovery rate, γ."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "γ_grid = collect(lb[2]:0.001:ub[2])\nθ_eval = [[0.001,γᵢ] for γᵢ in γ_grid]\nlfs_eval = gp.predict(θ_eval)\nfs_eval = invlogit.(lfs_eval[\"mean\"])\nfs_eval_uc = invlogit.(lfs_eval[\"mean\"] .+ 1.96 .* sqrt.(lfs_eval[\"unc\"]))\nfs_eval_lc = invlogit.(lfs_eval[\"mean\"] .- 1.96 .* sqrt.(lfs_eval[\"unc\"]))\nplot(γ_grid,\n     fs_eval,\n     xlabel = \"Recovery rate, γ\",\n     ylabel = \"Final size\",\n     label = \"Model\")\nplot!(γ_grid,\n      invlogit.(logit_final_size.(θ_eval)),\n      ribbon = (fs_eval .- fs_eval_lc, fs_eval_uc - fs_eval),\n      label = \"Surrogate\")"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History matching\n\n[History matching](https://mogp-emulator.readthedocs.io/en/latest/methods/thread/ThreadGenericHistoryMatching.html) is an approach used to learn about the inputs to a model using observations of the real system. The history matching process typically involves the use of expectations and variances of emulators, such as those generated by the Gaussian process emulator above. History matching seeks to identify regions of the input space that would give rise to acceptable matches between model output and observed data. 'Implausible' model outputs that are very different from the observed data are discarded, leaving a 'not ruled out yet' (NROY) set of input parameters.\n\nFirstly, we need some observations. We'll take the final size at the default parameter values `p` as our observation."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "obs = logit_final_size(p)\ninvlogit(obs)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate a `HistoryMatching` object, we pass the fitted Gaussian process, the observation, the coordinates at which we want to evaluate the fit and a threshold of implausibility that will be used to rule out parameter sets."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "hm = mogp.HistoryMatching(gp=gp,\n                          obs=obs,\n                          coords=np.array(θ_test),\n                          threshold=3.0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_NROY` method returns the indices of the NROY points; Python uses zero indexing, so we need to add one in order to use them in Julia."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "nroy_points = hm.get_NROY() .+ 1\nlength(nroy_points),n_test"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of parameter sets that are plausible decreased by an order of magnitude when history matching was applied. The below shows that the true values of β and γ are in the NROY set."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "x = [θᵢ[1] for θᵢ in θ_test]\ny = [θᵢ[2] for θᵢ in θ_test]\nl = @layout [a b]\npl1 = histogram(x[nroy_points],legend=false,xlim=(lb[1],ub[1]),bins=lb[1]:0.00005:ub[1],title=\"NROY values for β\")\nvline!(pl1,[p[1]])\npl2 = histogram(y[nroy_points],legend=false,xlim=(lb[2],ub[2]),bins=lb[2]:0.05:ub[2],title=\"NROY values for γ\")\nvline!(pl2,[p[2]])\nplot(pl1, pl2, layout = l)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, an iterative approach would be taken where the non-implausible parameter sets are used to generate a new set of parameter samples, from which a new emulator is fitted, and the new set of parameter values are filtered on the basis of the implausibility measure.\n\n## Benchmarking\n\nThe following demonstrates that the Gaussian process actually takes more time than this (admittedly simple) model, although given the parameter-dependent running time (as we run until steady state), simple summary statistics aren't that informative. The emulator does take significantly less memory, and this may be an important consideration in some settings."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@benchmark logit_final_size(p)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@benchmark gp.predict(p)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.2"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
