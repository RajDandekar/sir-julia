{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural ordinary differential equation model\nSimon Frost (@sdwfrost) 2022-03-31\n\nA neural ODE is an ODE where a neural network defines its derivative function. In this simple example, we train a neural ODE on a standard SIR model described by an ODE, and generate a forecast of the dynamics.\n\n## Libraries"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using OrdinaryDiffEq\nusing DiffEqFlux, Flux\nusing Random\nusing Plots;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(123);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transitions of ODE system\n\nTo assist in numerical stability, we consider the proportion of individuals in the population (`s,i,r`) rather than the number of individuals (`S,I,R`). As the neural ODEs are defined out-of-place (i.e. they return a new derivative `du`), we use this syntax for our 'true' ODE model."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function sir_ode(u,p,t)\n    (s,i,r) = u\n    (β,γ) = p\n    ds = -β*s*i\n    di = β*s*i - γ*i\n    dr = γ*i\n    [ds,di,dr]\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters, initial conditions, etc.\n\nNote that our parameter values and initial conditions reflect that in this example, we assume we are modeling proportions (i.e. `s+i+r=1`)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p = [0.5,0.25]\nu0 = [0.99, 0.01, 0.0]\ntspan = (0.0, 40.0)\nδt = 1;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the true model\n\nTo derive trajectories for training, we first solve the true model."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "solver = Rodas5();"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "sir_prob = ODEProblem(sir_ode, u0, tspan, p)\nsir_sol = solve(sir_prob, solver, saveat = δt);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple example assumes we have accurate values for all state variables, which we obtain from the solution of the ODE over the training time period.\n\nFor training, we use a short timespan of `(0,30)`, and will forecast for an additional 10 time units, with training using all three state variables every `δt` time units."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "train_time = 30.0\ntrain_data = Array(sir_sol(0:δt:train_time));"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a neural ODE\n\nTo define a neural ODE, we need to decide on an architecture. Here is a simple multilayer perceptron that takes three inputs (the three state variables `s,i,r`) and generates three outputs (the derivatives, `ds,di,dr`)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "nhidden = 8\nsir_node = FastChain(FastDense(3, nhidden, tanh),\n                     FastDense(nhidden, nhidden, tanh),\n                     FastDense(nhidden, nhidden, tanh),\n                     FastDense(nhidden, 3));"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are using a Flux.jl `FastChain`, we could write our neural ODE as follows (see [this page](https://diffeqflux.sciml.ai/dev/examples/neural_ode_sciml/) under 'Usage Without the Layer Function')."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p_ = Float64.(initial_params(sir_node));\nfunction dudt_sir_node(u,p,t)\n    s,i,r = u\n    ds,di,dr = ann_node([s,i,r],p)\n    [ds,di,dr]\nend\nprob_node = ODEProblem(dudt_sir_node, u0, tspan, p_);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, `DiffEqFlux.jl` offers a simpler interface where we can just pass a neural network, without generating the initial parameters and writing the gradient function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "tspan_train = (0,train_time)\nprob_node = NeuralODE(sir_node,\n                      tspan_train,\n                      solver,\n                      saveat=δt,\n                      sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP()))\nnump = length(prob_node.p)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following uses the sum of squared differences between the neural ODE predictions and the true state variables (`train_data`, above) as a loss function. As described [here](https://diffeqflux.sciml.ai/dev/examples/divergence/), it is important to be able to handle failed integrations."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function loss(p)\n    sol = prob_node(u0,p)\n    pred = Array(sol)\n    sum(abs2, (train_data .- pred)), pred\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a simple callback function that displays the current value of the loss every 50 steps. We'll keep an array of losses to plot later."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "const losses = []\ncallback = function (p, l, pred)\n    push!(losses, l)\n    numloss = length(losses)\n    if numloss % 50 == 0\n        display(\"Epoch: \" * string(numloss) * \" Loss: \" * string(l))\n    end\n    return false\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "res_node = DiffEqFlux.sciml_train(loss,\n                                   prob_node.p,\n                                   cb = callback);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "plot(losses, yaxis = :log, xaxis = :log, xlabel = \"Iterations\", ylabel = \"Loss\", legend = false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting\n\nWe generate a new problem with the parameters from the above fit."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prob_node = NeuralODE(sir_node,\n                      tspan_train,\n                      solver,\n                      saveat=δt,\n                      sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP()),\n                      p = res_node)\nsol_node = prob_node(u0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A plot of the dynamics shows a good fit."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "scatter(sir_sol, label=[\"True Susceptible\" \"True Infected\" \"True Recovered\"])\nplot!(sol_node, label=[\"Estimated Susceptible\" \"Estimated Infected\" \"Estimated Recovered\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting\n\nWe can also run the fitted model forward in time in order to assess its ability to forecast."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "tspan_test = (0.0, 40.0)\nprob_node_test = NeuralODE(sir_node,\n                      tspan_test,\n                      solver,\n                      saveat=δt,\n                      sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP()),\n                      p = res_node)\nsol_node_test = prob_node_test(u0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p_node = scatter(sol_node_test, legend = :topright, label=[\"True Susceptible\" \"True Infected\" \"True Recovered\"], title=\"Neural ODE Extrapolation: training until t=30\")\nplot!(p_node,sol_node_test, lw=5, label=[\"Estimated Susceptible\" \"Estimated Infected\" \"Estimated Recovered\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance for different initial conditions\n\nHere, we evaluate the fit for a different initial condition than that used for training."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "newu0 = [0.95, 0.05, 0.0]\nsir_prob_u0 = remake(sir_prob,u0=newu0)\nsir_sol_u0 = solve(sir_prob_u0, solver, saveat =  δt)\nnode_sol_u0 = prob_node(newu0)\np_node = scatter(sir_sol_u0, legend = :topright, label=[\"True Susceptible\" \"True Infected\" \"True Recovered\"], title=\"Neural ODE with different initial conditions\")\nplot!(p_node,node_sol_u0, lw=5, label=[\"Estimated Susceptible\" \"Estimated Infected\" \"Estimated Recovered\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the neural network is only training the derivatives, there is some dependency on the initial conditions used to generate the training data.\n\n## Performance with a shorter training dataset\n\nHere is an example of training with more limited data - training only on times `t=0:20`. The values of `abstol` and `reltol` are reduced in order to avoid numerical problems."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "tspan_train2 = (0.0,20.0)\nprob2 = ODEProblem(sir_ode, u0, tspan_train2, p)\nsol2 = solve(prob2, solver, saveat = δt)\ndata2 = Array(sol2)\nsolver2 = ExplicitRK()\nprob_node2 = NeuralODE(sir_node,\n                      tspan_train2,\n                      solver2,\n                      saveat=δt,\n                      sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP()))\nfunction loss2(p)\n    sol = prob_node2(u0,p)\n    pred = Array(sol)\n    sum(abs2, (data2 .- sol)), pred\nend\nconst losses2 = []\ncallback2 = function (p, l, pred)\n    push!(losses2, l)\n    numloss = length(losses2)\n    if numloss % 50 == 0\n        display(\"Epoch: \" * string(numloss) * \" Loss: \" * string(l))\n    end\n    return false\nend\nres_node2 = DiffEqFlux.sciml_train(loss2,\n                                   prob_node2.p,\n                                   cb = callback2);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now solve the new model over the full testing time period."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prob_node2_test = NeuralODE(sir_node,\n                      tspan_test,\n                      solver,\n                      saveat=δt,\n                      sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP()),\n                      p = res_node2)\nsol_node2_test = prob_node2_test(u0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A plot of the forecast shows that the model still predicts well in the very short term, but fails in the longer term due to more limited data, and shows unrealistic increases in susceptibles. With even more limited data (training up to `t=15.0`), the extrapolation becomes worse, and even shows negative population sizes."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p_node2 = scatter(sir_sol, legend = :topright, label=[\"True Susceptible\" \"True Infected\" \"True Recovered\"], title=\"Neural ODE Extrapolation: training until t=20\")\nplot!(p_node2, sol_node2_test, lw=5, label=[\"Estimated Susceptible\" \"Estimated Infected\" \"Estimated Recovered\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting with only a subset of state variables\n\nHere, we investigate what happens when we train a neural ODE on just a subset of state variables (here, `I`)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function loss3(p)\n    sol = prob_node(u0,p)\n    pred = Array(sol)\n    sum(abs2, (train_data[2,:] .- sol[2,:])), pred\nend\nconst losses3 = []\ncallback3 = function (p, l, pred)\n    push!(losses3, l)\n    numloss = length(losses3)\n    if numloss % 50 ==0\n        display(\"Epoch: \" * string(numloss) * \" Loss: \" * string(l))\n    end\n    return false\nend\nres_node3 = DiffEqFlux.sciml_train(loss3,\n                                   prob_node.p,\n                                   cb = callback3);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "prob_node3_test = NeuralODE(sir_node,\n                      tspan_test,\n                      solver,\n                      saveat=δt,\n                      sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP()),\n                      p = res_node3)\nsol_node3_test = prob_node3_test(u0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "p_node3 = scatter(sir_sol, legend = :topright, label=[\"True Susceptible\" \"True Infected\" \"True Recovered\"], title=\"Neural ODE Extrapolation: training on I(t) until t=30\")\nplot!(p_node3, sol_node3_test, lw=5, label=[\"Estimated Susceptible\" \"Estimated Infected\" \"Estimated Recovered\"])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dynamics for the observed variable, `I` are fitted well; however, not surprisingly, in the absence of other information, the dynamics of the hidden variables, `S` and `R` are not.\n\n## Discussion\n\nNeural ODEs provide a way to fit and extrapolate in a data-driven way, and they perform well, at least in terms of fit and short-term forecasts, for this simple example. With more limited data, the goodness-of-fit to the training data may be misleading. Numerical issues can also arise in the fitting of neural ODEs. Potential solutions include changing the solver and decreasing the tolerances. In addition, we rarely have access to all the state variables of a system. Many of these deficiencies can be addressed through combining neural network approaches with domain-specific knowledge e.g. using [universal differential equations](https://arxiv.org/abs/2001.04385)."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.6.2"
    },
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6.2",
      "language": "julia"
    }
  },
  "nbformat": 4
}
